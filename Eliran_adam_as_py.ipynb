{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import qas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import pandas\n",
    "import joblib\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from qas.constants import CORPUS_DIR, EN_MODEL_MD\n",
    "from qas.corpus.data import QUESTION_CLASSIFICATION_TRAINING_DATA, QUESTION_CLASSIFICATION_MODEL\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def pre_process(dta):\n",
    "    return pandas.get_dummies(dta)\n",
    "\n",
    "\n",
    "def remove_irrelevant_features(df_question):\n",
    "    df_question_class = df_question.pop('Class')\n",
    "\n",
    "    df_question.pop('Question')\n",
    "    df_question.pop('WH-Bigram')\n",
    "\n",
    "    return df_question_class\n",
    "\n",
    "\n",
    "def transform_data_matrix(df_question_train, df_question_predict):\n",
    "\n",
    "    df_question_train_columns = list(df_question_train.columns)\n",
    "    df_question_predict_columns = list(df_question_predict.columns)\n",
    "\n",
    "    df_question_trans_columns = list(set(df_question_train_columns + df_question_predict_columns))\n",
    "\n",
    "    logger.debug(\"Union Columns: {0}\".format(len(df_question_trans_columns)))\n",
    "\n",
    "    trans_data_train = {}\n",
    "\n",
    "    for feature in df_question_trans_columns:\n",
    "        if feature not in df_question_train:\n",
    "            trans_data_train[feature] = [0 for i in range(len(df_question_train.index))]\n",
    "        else:\n",
    "            trans_data_train[feature] = list(df_question_train[feature])\n",
    "\n",
    "    df_question_train = pandas.DataFrame(trans_data_train)\n",
    "    logger.debug(\"Training data: {0}\".format(df_question_train.shape))\n",
    "    df_question_train = csr_matrix(df_question_train)\n",
    "\n",
    "    trans_data_predict = {}\n",
    "\n",
    "    for feature in trans_data_train:\n",
    "        if feature not in df_question_predict:\n",
    "            trans_data_predict[feature] = 0\n",
    "        else:\n",
    "            trans_data_predict[feature] = list(df_question_predict[feature])  # KeyError\n",
    "\n",
    "    df_question_predict = pandas.DataFrame(trans_data_predict)\n",
    "    logger.debug(\"Target data: {0}\".format(df_question_predict.shape))\n",
    "    df_question_predict = csr_matrix(df_question_predict)\n",
    "\n",
    "    return df_question_train, df_question_predict\n",
    "\n",
    "\n",
    "def naive_bayes_classifier(x_train, y, x_predict):\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(x_train, y)\n",
    "    prediction = gnb.predict(x_predict)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def support_vector_machine(df_question_train, df_question_class, df_question_predict):\n",
    "    lin_clf = LinearSVC()\n",
    "    lin_clf.fit(df_question_train, df_question_class)\n",
    "    prediction = lin_clf.predict(df_question_predict)\n",
    "    return prediction, lin_clf\n",
    "\n",
    "\n",
    "def predict_question_class(question_clf, df_question_predict):\n",
    "    return question_clf.predict(df_question_predict), question_clf\n",
    "\n",
    "\n",
    "def load_classifier_model(model_type=\"linearSVC\"):\n",
    "\n",
    "    # HELP: Not using the persistent classifier. SVC fails when it encounters previously unseen features at training.\n",
    "    # Refer the comment in query_container\n",
    "\n",
    "    training_model_path = os.path.join(CORPUS_DIR, QUESTION_CLASSIFICATION_MODEL)\n",
    "\n",
    "    if model_type == \"linearSVC\":\n",
    "        return joblib.load(training_model_path)\n",
    "\n",
    "\n",
    "def get_question_predict_data(en_doc=None, df_question_test=None):\n",
    "\n",
    "    if df_question_test is None:\n",
    "        # currently only supports single sentence classification\n",
    "        sentence_list = list(en_doc.sents)[0:1]\n",
    "\n",
    "    else:\n",
    "        sentence_list = df_question_test[\"Question\"].tolist()\n",
    "\n",
    "        import spacy\n",
    "        en_nlp = spacy.load(EN_MODEL_MD)\n",
    "\n",
    "    question_data_frame = []\n",
    "\n",
    "    for sentence in sentence_list:\n",
    "\n",
    "        wh_bi_gram = []\n",
    "        root_token, wh_pos, wh_nbor_pos, wh_word = [\"\"] * 4\n",
    "\n",
    "        if df_question_test is not None:\n",
    "            en_doc = en_nlp(u'' + sentence)\n",
    "            sentence = list(en_doc.sents)[0]\n",
    "\n",
    "        for token in sentence:\n",
    "\n",
    "            if token.tag_ == \"WDT\" or token.tag_ == \"WP\" or token.tag_ == \"WP$\" or token.tag_ == \"WRB\":\n",
    "                wh_pos = token.tag_\n",
    "                wh_word = token.text\n",
    "                wh_bi_gram.append(token.text)\n",
    "                wh_bi_gram.append(str(en_doc[token.i + 1]))\n",
    "                wh_nbor_pos = en_doc[token.i + 1].tag_\n",
    "\n",
    "            if token.dep_ == \"ROOT\":\n",
    "                root_token = token.tag_\n",
    "\n",
    "        question_data_frame_obj = {'WH': wh_word, 'WH-POS': wh_pos, 'WH-NBOR-POS': wh_nbor_pos, 'Root-POS': root_token}\n",
    "        question_data_frame.append(question_data_frame_obj)\n",
    "        logger.debug(\"WH : {0} | WH-POS : {1} | WH-NBOR-POS : {2} | Root-POS : {3}\"\n",
    "                     .format(wh_word, wh_pos, wh_nbor_pos, root_token))\n",
    "\n",
    "    df_question = pandas.DataFrame(question_data_frame)\n",
    "\n",
    "    return df_question\n",
    "\n",
    "\n",
    "def classify_question(en_doc=None, df_question_train=None, df_question_test=None):\n",
    "    \"\"\" Determine whether this is a who, what, when, where or why question \"\"\"\n",
    "\n",
    "    if df_question_train is None:\n",
    "        training_data_path = os.path.join(CORPUS_DIR, QUESTION_CLASSIFICATION_TRAINING_DATA)\n",
    "        df_question_train = pandas.read_csv(training_data_path, sep='|', header=0)\n",
    "\n",
    "    df_question_class = remove_irrelevant_features(df_question_train)\n",
    "\n",
    "    if df_question_test is None:\n",
    "        df_question_predict = get_question_predict_data(en_doc=en_doc)\n",
    "    else:\n",
    "        df_question_predict = get_question_predict_data(df_question_test=df_question_test)\n",
    "\n",
    "    df_question_train = pre_process(df_question_train)\n",
    "    df_question_predict = pre_process(df_question_predict)\n",
    "\n",
    "    df_question_train, df_question_predict = transform_data_matrix(df_question_train, df_question_predict)\n",
    "\n",
    "    question_clf = load_classifier_model()\n",
    "\n",
    "    logger.debug(\"Classifier: {0}\".format(question_clf))\n",
    "\n",
    "    predicted_class, svc_clf = support_vector_machine(df_question_train, df_question_class, df_question_predict)\n",
    "\n",
    "    if df_question_test is not None:\n",
    "        return predicted_class, svc_clf, df_question_class, df_question_train\n",
    "    else:\n",
    "        return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:WH : Who | WH-POS : WP | WH-NBOR-POS : VBZ | Root-POS : VBZ\n",
      "DEBUG:__main__:Union Columns: 63\n",
      "DEBUG:__main__:Training data: (5365, 63)\n",
      "DEBUG:__main__:Target data: (1, 63)\n",
      "DEBUG:__main__:Classifier: ['ABBR' 'DESC' 'ENTY' 'HUM' 'LOC' 'NUM']\n",
      "INFO:__main__:Class: ['HUM']\n",
      "INFO:__main__:Total prediction time : 29.93218684196472\n"
     ]
    }
   ],
   "source": [
    "    import spacy\n",
    "    from time import time\n",
    "\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    start_time = time()\n",
    "    en_nlp_l = spacy.load(EN_MODEL_MD)\n",
    "\n",
    "    question = 'Who is Linus Torvalds ?'\n",
    "    en_doc_l = en_nlp_l(u'' + question)\n",
    "\n",
    "    question_class = classify_question(en_doc_l)\n",
    "\n",
    "    logger.info(\"Class: {0}\".format(question_class))\n",
    "\n",
    "    end_time = time()\n",
    "    logger.info(\"Total prediction time : {0}\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       question num\n",
      "0         Can an affidavit be used in Beit Din?  10\n",
      "1    How can I write HTML and send as an email?  15\n",
      "2       How do I remove a Facebook app request?  14\n",
      "3          How do you grapple in Dead Rising 3?   1\n",
      "4  How do you make a binary image in Photoshop?   1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nlp(language, lite, lang_model=\"\"):\n",
    "    err_msg = \"Language model {0} not found. Please, refer https://spacy.io/usage/models\"\n",
    "    nlp = None\n",
    "\n",
    "    if not lang_model == \"\" and not lang_model == \"en\":\n",
    "\n",
    "        try:\n",
    "            nlp = spacy.load(lang_model)\n",
    "        except ImportError:\n",
    "            print(err_msg.format(lang_model))\n",
    "            raise\n",
    "\n",
    "    elif language == 'en':\n",
    "\n",
    "        if lite:\n",
    "            nlp = spacy.load(EN_MODEL_DEFAULT)\n",
    "        else:\n",
    "\n",
    "            try:\n",
    "                nlp = spacy.load(EN_MODEL_MD)\n",
    "            except (ImportError, OSError):\n",
    "                print(err_msg.format(EN_MODEL_MD))\n",
    "                print('Using default language model')\n",
    "                nlp = get_default_model(EN_MODEL_DEFAULT)\n",
    "\n",
    "    elif not language == 'en':\n",
    "        print('Currently only English language is supported. '\n",
    "              'Please contribute to https://github.com/5hirish/adam_qas to add your language.')\n",
    "        sys.exit(0)\n",
    "\n",
    "    return nlp\n",
    "\n",
    "class QasInit:\n",
    "\n",
    "    nlp = None\n",
    "    language = \"en\"\n",
    "    lang_model = None\n",
    "    search_depth = 3\n",
    "    lite = False\n",
    "\n",
    "    question_doc = None\n",
    "\n",
    "    question_class = \"\"\n",
    "    question_keywords = None\n",
    "    query = None\n",
    "\n",
    "    candidate_answers = None\n",
    "\n",
    "    def __init__(self, language, search_depth, lite, lang_model=\"\"):\n",
    "        self.language = language\n",
    "        self.search_depth = search_depth\n",
    "        self.lite = lite\n",
    "        self.lang_model = lang_model\n",
    "        self.nlp = get_nlp(self.language, self.lite, self.lang_model)\n",
    "\n",
    "    def get_question_doc(self, question):\n",
    "\n",
    "        self.question_doc = self.nlp(u'' + question)\n",
    "\n",
    "        return self.question_doc\n",
    "\n",
    "    def process_question(self):\n",
    "\n",
    "        self.question_class = classify_question(self.question_doc)\n",
    "        _logger.info(\"Question Class: {}\".format(self.question_class))\n",
    "\n",
    "        self.question_keywords = extract_features(self.question_class, self.question_doc)\n",
    "        _logger.info(\"Question Features: {}\".format(self.question_keywords))\n",
    "\n",
    "        self.query = construct_query(self.question_keywords, self.question_doc)\n",
    "        _logger.info(\"Query: {}\".format(self.query))\n",
    "\n",
    "    def process_answer(self):\n",
    "\n",
    "        _logger.info(\"Retrieving {} Wikipedia pages...\".format(self.search_depth))\n",
    "        search_wikipedia(self.question_keywords, self.search_depth)\n",
    "\n",
    "        # Anaphora Resolution\n",
    "        wiki_pages = search_rank(self.query)\n",
    "        _logger.info(\"Pages retrieved: {}\".format(len(wiki_pages)))\n",
    "\n",
    "        self.candidate_answers, keywords = get_candidate_answers(self.query, wiki_pages, self.nlp)\n",
    "        _logger.info(\"Candidate answers ({}):\\n{}\".format(len(self.candidate_answers), '\\n'.join(self.candidate_answers)))\n",
    "\n",
    "        return \" \".join(self.candidate_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:qas.classifier.question_classifier:WH :  | WH-POS :  | WH-NBOR-POS :  | Root-POS : VBN\n",
      "DEBUG:qas.classifier.question_classifier:Union Columns: 66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can an affidavit be used in Beit Din?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:qas.classifier.question_classifier:Training data: (5365, 66)\n",
      "DEBUG:qas.classifier.question_classifier:Target data: (1, 66)\n",
      "DEBUG:qas.classifier.question_classifier:Classifier: ['ABBR' 'DESC' 'ENTY' 'HUM' 'LOC' 'NUM']\n",
      "INFO:__main__:Question Class: ['HUM']\n",
      "DEBUG:qas.feature_extractor:Compound Noun:affidavit DEP nsubjpass\n",
      "DEBUG:qas.feature_extractor:Compound Noun:Din DEP pobj\n",
      "INFO:__main__:Question Features: ['affidavit', 'Beit Din', 'use']\n",
      "INFO:__main__:Query: [{Features: ['affidavit', 'Beit Din', 'use'] ,Conjunction: [] ,Negations: [] ,Marker: []}]\n",
      "DEBUG:qas.classifier.question_classifier:WH : How | WH-POS : WRB | WH-NBOR-POS : MD | Root-POS : VB\n",
      "DEBUG:qas.classifier.question_classifier:Union Columns: 63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I write HTML and send as an email?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:qas.classifier.question_classifier:Training data: (5365, 63)\n",
      "DEBUG:qas.classifier.question_classifier:Target data: (1, 63)\n",
      "DEBUG:qas.classifier.question_classifier:Classifier: ['ABBR' 'DESC' 'ENTY' 'HUM' 'LOC' 'NUM']\n",
      "INFO:__main__:Question Class: ['DESC']\n",
      "DEBUG:qas.feature_extractor:Compound Noun:HTML DEP dobj\n",
      "DEBUG:qas.feature_extractor:Compound Noun:email DEP pobj\n",
      "INFO:__main__:Question Features: ['HTML', 'email', 'write']\n",
      "DEBUG:qas.query_const:Conjunction: `and` at 5\n",
      "DEBUG:qas.query_const:Conjuncting: ['write', 'send']\n",
      "INFO:__main__:Query: [{Features: ['HTML', 'email', 'write'] ,Conjunction: [['write', 'send'], 'and'] ,Negations: [] ,Marker: []}]\n",
      "DEBUG:qas.classifier.question_classifier:WH : How | WH-POS : WRB | WH-NBOR-POS : VBP | Root-POS : VB\n",
      "DEBUG:qas.classifier.question_classifier:Union Columns: 63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I remove a Facebook app request?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:qas.classifier.question_classifier:Training data: (5365, 63)\n",
      "DEBUG:qas.classifier.question_classifier:Target data: (1, 63)\n",
      "DEBUG:qas.classifier.question_classifier:Classifier: ['ABBR' 'DESC' 'ENTY' 'HUM' 'LOC' 'NUM']\n",
      "INFO:__main__:Question Class: ['DESC']\n",
      "DEBUG:qas.feature_extractor:Compound Noun:request DEP dobj\n",
      "INFO:__main__:Question Features: ['Facebook app request', 'remove']\n",
      "INFO:__main__:Query: [{Features: ['Facebook app request', 'remove'] ,Conjunction: [] ,Negations: [] ,Marker: []}]\n",
      "DEBUG:qas.classifier.question_classifier:WH : How | WH-POS : WRB | WH-NBOR-POS : VBP | Root-POS : VB\n",
      "DEBUG:qas.classifier.question_classifier:Union Columns: 63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do you grapple in Dead Rising 3?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:qas.classifier.question_classifier:Training data: (5365, 63)\n",
      "DEBUG:qas.classifier.question_classifier:Target data: (1, 63)\n",
      "DEBUG:qas.classifier.question_classifier:Classifier: ['ABBR' 'DESC' 'ENTY' 'HUM' 'LOC' 'NUM']\n",
      "INFO:__main__:Question Class: ['DESC']\n",
      "INFO:__main__:Question Features: ['3', 'grapple']\n",
      "INFO:__main__:Query: [{Features: ['3', 'grapple'] ,Conjunction: [] ,Negations: [] ,Marker: []}]\n",
      "DEBUG:qas.classifier.question_classifier:WH : How | WH-POS : WRB | WH-NBOR-POS : VBP | Root-POS : VB\n",
      "DEBUG:qas.classifier.question_classifier:Union Columns: 63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do you make a binary image in Photoshop?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:qas.classifier.question_classifier:Training data: (5365, 63)\n",
      "DEBUG:qas.classifier.question_classifier:Target data: (1, 63)\n",
      "DEBUG:qas.classifier.question_classifier:Classifier: ['ABBR' 'DESC' 'ENTY' 'HUM' 'LOC' 'NUM']\n",
      "INFO:__main__:Question Class: ['DESC']\n",
      "DEBUG:qas.feature_extractor:Compound Noun:image DEP dobj\n",
      "DEBUG:qas.feature_extractor:Compound Noun:Photoshop DEP pobj\n",
      "INFO:__main__:Question Features: ['image', 'Photoshop', 'make']\n",
      "INFO:__main__:Query: [{Features: ['image', 'Photoshop', 'make'] ,Conjunction: [] ,Negations: [] ,Marker: []}]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "def main(argv)\n",
    "    _logger = logging.getLogger(\"__main__\")\n",
    "    import spacy\n",
    "    from qas.candidate_ans import get_candidate_answers\n",
    "    from qas.classifier.question_classifier import classify_question\n",
    "    from qas.constants import EN_MODEL_MD, EN_MODEL_DEFAULT, EN_MODEL_SM\n",
    "    from qas.doc_search_rank import search_rank\n",
    "    from qas.feature_extractor import extract_features\n",
    "    from qas.query_const import construct_query\n",
    "    from qas.wiki.wiki_search import search_wikipedia\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    en_nlp_l = spacy.load(EN_MODEL_MD)\n",
    "    for question in df[\"question\"]:\n",
    "        qass = QasInit(search_depth=3 ,language=QasInit.language, lite=False, lang_model=QasInit.language)\n",
    "        print(question)\n",
    "        qass.get_question_doc(question)\n",
    "        qass.process_question()\n",
    "    #     answer = qass.process_answer()\n",
    "        #\n",
    "    #     print(\"\\n\\n** Your answer:\\n {}\".format(answer))\n",
    "    #     en_doc_l = en_nlp_l(u'' + question)\n",
    "\n",
    "    #     question_class = classify_question(en_doc_l)\n",
    "    #     logger.info(\"Class: {0}\".format(question_class))\n",
    "\n",
    "    #     self.question_keywords = extract_features(self.question_class, self.question_doc)\n",
    "    #     _logger.info(\"Question Features: {}\".format(self.question_keywords))\n",
    "\n",
    "    #     self.query = construct_query(self.question_keywords, self.question_doc)\n",
    "    #     _logger.info(\"Query: {}\".format(self.query))\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
